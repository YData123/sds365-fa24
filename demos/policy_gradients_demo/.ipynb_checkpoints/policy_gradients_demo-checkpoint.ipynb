{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient methods: Minimal example\n",
    "\n",
    "In this notebook, we illustrate the core concepts of policy gradient methods using \n",
    "a simple example.\n",
    "\n",
    "Image that you're throwing darts at a dart board--blindfolded. After each darts you throw, you are given a reward that depends on how close to the bullseye you are. Based on this information, you explore the space to learn where the target is. Each throw lands at a location that is a normal random variable with a given mean (your intended target) and variance (how accurate you are).\n",
    "\n",
    "This corresponds to a policy $\\pi_\\theta(a \\;|\\; s)$ that is distributed as a Gaussian \n",
    "random variable $a \\sim N(\\mu(s), \\sigma^2(s))$. The mean $\\mu(s)$ and the standard deviation $\\sigma(s)$ will be the outputs of a neural network, much like for a variational autoencoder. The parameters $\\theta$ are the weights in the neural network. To compute the gradient of the expected reward, we use automatic differentiation, building up the objective function using tensorflow operations.\n",
    "\n",
    "The neural network that implements the policy is called the \"actor network\" since it \n",
    "is responsible for deciding which actions are taken. In this particular example, the state is simply a fixed constant.\n",
    "\n",
    "Our example is taken from [from this blog](https://towardsdatascience.com/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Construct the actor network with mu and sigma as output\"\"\"\n",
    "def construct_actor_network():\n",
    "    inputs = layers.Input(shape=(1,))     \n",
    "    hidden1 = layers.Dense(5, activation=\"relu\")(inputs)\n",
    "    hidden2 = layers.Dense(5, activation=\"relu\")(hidden1)\n",
    "    mu = layers.Dense(1, activation=\"linear\")(hidden2) \n",
    "    sigma = layers.Dense(1, activation=\"sigmoid\")(hidden2) \n",
    "    actor_network = keras.Model(inputs=inputs, outputs=[mu,sigma]) \n",
    "    return actor_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Weighted Gaussian log likelihood loss function\"\"\"\n",
    "def gaussian_loss(state, action, reward): \n",
    "    # Obtain mu and sigma from actor network\n",
    "    nn_mu, nn_sigma = actor_network(state)\n",
    "        \n",
    "    # pdf of Gaussian distribution\n",
    "    pdf_value = tf.exp(-0.5 *((action - nn_mu) / (nn_sigma))**2) *\\\n",
    "         1/(nn_sigma*tf.sqrt(2 *np.pi))\n",
    "\n",
    "    # Compute log probability\n",
    "    log_probability = tf.math.log(pdf_value + 1e-5)\n",
    "        \n",
    "    # Compute weighted loss\n",
    "    loss_actor = - reward * log_probability\n",
    "    return loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot output\"\"\"\n",
    "def plot():\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    epoch_ar.append(int(i))\n",
    "    mu_ar.append(float(mu))\n",
    "    sigma_ar.append(float(sigma))\n",
    "    reward_ar.append(float(reward))\n",
    "    target_ar.append(float(mu_target))\n",
    "    \n",
    "    ax.plot(epoch_ar,mu_ar,label='mu')\n",
    "    ax.plot(epoch_ar,sigma_ar,label='sigma')\n",
    "    ax.plot(epoch_ar,reward_ar,label='reward')\n",
    "    ax.plot(epoch_ar,target_ar,label='target')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Parameter value')\n",
    "    plt.xlim((0, (int(i/1000)+1)*1000))\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize fixed state\n",
    "state = tf.constant([[1.0]])\n",
    "\n",
    "# Define properties reward function\n",
    "mu_target = 3.0\n",
    "target_precision = 0.10\n",
    "max_reward = 1.0\n",
    "\n",
    "# Create actor network\n",
    "actor_network = construct_actor_network()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "# Initialize arrays for plot\n",
    "epoch_ar = []\n",
    "mu_ar = []\n",
    "sigma_ar=[]\n",
    "reward_ar=[]\n",
    "target_ar=[]\n",
    "\n",
    "for i in range(10000):    \n",
    "    \n",
    "    # Obtain mu and sigma from network\n",
    "    mu, sigma = actor_network(state)\n",
    "    \n",
    "    # Draw action from normal distribution\n",
    "    action = tf.random.normal([1], mean=mu, stddev=sigma)\n",
    "   \n",
    "    # Compute reward\n",
    "    reward = max_reward/ max(target_precision, abs(mu_target-action)) * target_precision\n",
    "\n",
    "    # Update network weights  \n",
    "    with tf.GradientTape() as tape:   \n",
    "        # Compute Gaussian loss\n",
    "        loss_value = gaussian_loss(state, action, reward)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss_value, actor_network.trainable_variables)\n",
    " \n",
    "        #Apply gradients to update network weights\n",
    "        opt.apply_gradients(zip(grads, actor_network.trainable_variables))\n",
    "        \n",
    "        \n",
    "    # Update console output and plot\n",
    "    if np.mod(i, 50) == 0:\n",
    "        clear_output(wait=True)\n",
    "        plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
